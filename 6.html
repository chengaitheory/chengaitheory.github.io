<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<meta name="generator" content="jemdoc, see http://jemdoc.jaboc.net/" />
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<link rel="stylesheet" href="jemdoc.css" type="text/css" />

    
<title>Research Statement</title>
<style>
    p {
        line-height: 1.8;
        font-family: verdana;
    }
    figcaption {
        padding: 5px;
        font-size: 0.85em;
        border: none;
        background: transparent;
        word-wrap:normal;
        text-align: center;
        font-family: verdana;
        line-height: 1.8;
    }
</style>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});
    </script>
<script type="text/javascript"
    src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
    </script>

</head>
<body>
<table summary="Table for page layout." id="tlayout">
<tr valign="top">
<td id="layout-menu">
<div class="menu-category">Guang Cheng</div>
<div class="menu-item"><a href="index.html">Home</a></div>
<div class="menu-item"><a href="2.html">People</a></div>
<div class="menu-item"><a href="1.html" class="current">Research</a></div>
<div class="menu-item"><a href="3.html">Batmen meeting</a></div>
<div class="menu-item">Data Science Seminar</a></div>
</td>
<td id="layout-content">
<div id="toptitle">
<h1>Research Summary</h1>
</div>

<p>
This page summarizes Dr. Cheng's main contributions in mathematical statistics, high dimensional statistics and big data theory, plus his ongoing works in deep learning.
</p>
<ul>
<li> <p><font style="font-weight:bold">Mathematical Statistics</font>: Together with his collaborators, Dr. Cheng has established bootstrap consistency for general semiparametric models <a href="https://projecteuclid.org/euclid.aos/1279638543">(Cheng & Huang, 2010)</a>, developed a first set of local and global inference procedures for smoothing spline models <a href="https://projecteuclid.org/euclid.aos/1384871347">(Shang & Cheng, 2013)</a>, together with the corresponding nonparametric Bayesian inferences <a href="https://academic.oup.com/imaiai/article/7/3/509/4772758">(Shang & Cheng, 2018).</a> The second work was established by inventing a new technical tool called as "functional Bahadur representation," and the third work rigorously proved that the “data washes out prior” phenomenon still exists in the nonparametric regime under a Bayesian version of “under-smoothing” condition.  All three representative works provide fundamental inferential theory in general semi- or non-parametric context;</p>
</li>

<li> <p><font style="font-weight:bold">High Dimensional Statistics</font>: Together with his collaborators, Dr. Cheng has developed Gaussian approximation results for high dimensional dependent data  <a href="https://arxiv.org/abs/1406.1037">(Zhang & Cheng, 2014</a> and <a href="https://projecteuclid.org/euclid.bj/1522051220">Zhang & Cheng, 2018)</a>. A particularly important application is to make simultaneous inferences on high dimensional linear models <a href="https://amstat.tandfonline.com/doi/abs/10.1080/01621459.2016.1166114">(Zhang & Cheng, 2017)</a>. Another work <a href="https://arxiv.org/abs/1612.05906"> (Yu et al., 2018)</a> revealed an &ldquo;one-way interaction phenomenon&rdquo; in estimating high dimensional partially linear additive models. Specifically, minimax risk for smooth nonparametric estimation can be slowed down to the sparse estimation rate whenever the smoothness of the nonparametric component or dimensionality of the parametric component is sufficiently large. The phase transition diagram is demonstrated in Figure <a href="#f3">1</a> below;</p>

<figure align="middle" id="f3">
<img src="plot.png" height="250">
<figcaption>Figure 1: <a href="https://arxiv.org/abs/1612.05906">Yu et al. (2018)</a> considered estimating $Y=X'\beta_0+f_0(Z)+g_0(U)+\epsilon$, where $\beta_0\in\mathbb R^p$ with at most $s_0$ non-zero elements and the smoothness of $f_0$ is $\alpha$. The minimax lower bound for estimating $f_0$ is $n^{-\alpha/(2\alpha+1)}$ when $\alpha, p, s_0$ and $n$ fall into smooth regime. Otherwise, the minimax lower bound becomes $s_0\log(p/s_0)/n$ in the sparse regime.<figcaption>
</figure>
</li>

<li> <p><font style="font-weight:bold">Big Data Theory</font>: The increased availability of massive data sets significantly helps discover subtle patterns in their distributions, but also imposes overwhelming computational challenges. A question of practical relevance is how one should allocate a limited computational budget to obtain the best possible statistically-based solution. This requires a <em>sharp</em> characterization of <span style="color:blue">statistical-and-computational trade-off</span>. We use the divide-and-conquer algorithm for smoothing spline as a prototypical example to illustrate this point. <a href="http://jmlr.org/papers/v18/16-289.html">Shang & Cheng (2017)</a> discovered a phase transition phenomenon for the number of distributed computing units that ends up being a simple proxy for computing cost. Specifically, a sharp upper bound for the number of deployed machines is established: when the number is below this bound, statistical optimality (in terms of estimation or testing) is achievable; otherwise, statistical optimality becomes impossible. These sharp bounds partly capture intrinsic computational limits of the distributed algorithm from a statistical perspective. An even deeper insight is that sample splitting may be viewed as an alternative form of regularization, playing a similar role as smoothing parameter. Please see Figure <a href="#f1">1</a> for an illustration of this point. </p>
<figure align="middle" id="f1">
<img src="sharps_estimation.png" height="250">
<img src="sharps_testing.png" height="250">
    <figcaption>Figure 2: <a href="http://jmlr.org/papers/v18/16-289.html">Shang & Cheng (2017)</a> considered estimation and testing of $f$ in $Y=f(X)+\epsilon$ via divide-and-conquer algorithm. Define $N$, $\lambda$ and $s$ as the entire sample size, smoothing parameter used in each machine and the total number of machines, respectively. Two lines indicate the choices of $s\asymp N^a$ and $\lambda\asymp N^{-b}$, leading to minimax optimal estimation rate (left) and minimax optimal testing rate (right). Whereas $(a,b)$'s outside these two lines lead to suboptimal rates.<figcaption>
</figure>
<p>
The statistical-and-computational tradeoff phenomenon has also been observed in the random projection method <a href="https://arxiv.org/abs/1802.06308">(Liu et al., 2018)</a> and early stopping method <a href="https://arxiv.org/abs/1805.09950">(Liu & Cheng, 2018)</a> in terms of the minimal surrogate dataset (after random projection) and the optimal number of iterations, respectively. Please see Figure <a href="#f2">3</a>.
</p>
<figure align="middle" id="f2">
<img src="phase_tran.png" height="250">
<img src="est_test.png" height="250">
<figcaption>Figure 3: <a href="https://arxiv.org/abs/1802.06308">Liu et al. (2018)</a> considered nonparametric testing for $f$ in $Y=f(X)+\epsilon$ via random projection. The horizontal axis $\lambda$ is the smoothing parameter used in kernel ridge regression, and the vertical axis $s$ is the size of surrogate dataset. SWDS means the strength of the weakest detectable signals. The vertical line labeled by &ldquo;optimal&rdquo; indicates the choices of $\lambda$ and the corresponding smallest sample size $s^*$ that achieve the smallest SWDS. The right plot illustrates different &ldquo;bias-variance&rdquo; tradeoff in testing and estimation, thus leading to different stopping time: $\widetilde T$ v.s. $T^*$ <a href="https://arxiv.org/abs/1805.0995">(Liu & Cheng, 2018)</a>.<figcaption>
</figure>

<p>
<span style="color:blue">Large dimensional tensor data</span>
 is a particular type of big data Dr. Cheng has investigated in depth, e.g., sparse tensor decomposition <a href="https://rss.onlinelibrary.wiley.com/doi/full/10.1111/rssb.12190">(Sun et al., 2017)</a> and tensor recovery <a href="https://arxiv.org/abs/1801.09326">(Hao et al., 2018)</a>. The latter aims to recover an unknown tensor $\mathcal{B}$ from different types of sketchings. Specifically, we consider
\begin{eqnarray*}
\label{model}
y_i = \langle\mathcal{B}, \mathcal{X}_i\rangle+\epsilon_i, \ i=1,\ldots, n.
\end{eqnarray*}
Here, $\mathcal{X}_i$ is an observable sketching tensor with either symmetric or non-symmetric form. The former form corresponds to compressed measurements for high-order tensor data and the latter to a tensor representation of high-order interaction; see Figure <a href="#f4">4</a> for an illustration. By imposing the low rank structure of $\mathcal{B}$, we sketch the tensor along three different modes individually, which leads to the &ldquo;cubic sketching&rdquo; method. We prove that the block-wise gradient descent estimate converges to global optima with a geometric rate in optimization error and minimax optimal rate in statistical error.
</p>

<figure align="middle" id="f4">
<img src="illustration.png" height="250">
<img src="3.png" height="250">
<figcaption>Figure 4: Illustration for tensor image compressing and tensor formulation of interaction terms.<figcaption>
</figure>
</li>



</ul>
</p>
<p>
In the new area of <font style="font-weight:bold">Deep Learning</font>, the first attempt Dr. Cheng has made is to turn the well known generative adversarial network (GAN) into a statistical sampling procedure called as "<span style="color:blue">Stein Neural Sampler</span>" <a href="https://arxiv.org/abs/1810.03545">(Hu et al., 2018)</a>. Such a GAN-based sampling method takes advantage of the large capacity of neural network. Theoretical and empirical results suggest that our samplers are asymptotically correct, experience less convergence issue and generating samples instantaneously. Please see Figure <a href="#f5">5</a> for a simple comparison with other methods.
</p>
<figure align="middle" id="f5">
<img src="8Gaussian.png" height="250">
<figcaption>Figure 5: 2D Gaussian mixture with 8 components. The red dashed lines are the target density and the green dots are generated samples at 0, 1000, 2000, 5000, 10000 iterations, respectively.<figcaption>
</figure>

<p>
In deep learning, it is observed that carefully designed deep neural networks achieve small testing error even the training error is close to zero. One possible explanation is that over-fitting may greatly reduce the estimation bias, while not increase the estimation variance too much. <a href="https://arxiv.org/pdf/1810.02814.pdf">Xing et al. (2018)</a> tried to understand this over-fitting phenomenon using the simple nearest neighbor algorithms under the interpolation constraint; see the left panel of Figure <a href="#f6">6</a>. We proved that such interpolated algorithms can still achieve the minimax optimal rate in both regression and classification regimes as long as weights are carefully chosen. Moreover, in quite a few scenarios, the interpolated algorithms can be even better than the traditional $k$-nearest neighbor method; see the right panel of Figure <a href="#f6">6</a>. 
</p>
<figure align="middle" id="f6">
<img src="toy_cut.png" height="250">
<img src="class.png" height="250">
    <figcaption>Figure 6: In the left plot, $\phi(t)$ represents the weight function in the nearest neighbor regression estimate. In the right plot for classifying $N(0, I_5)$ v.s. $N(\gamma 1, I_5)$, solid line and dashed line represent the optimal excess risk for classification of $k$-NN and interpolated-NN, respectively.<figcaption>
</figure>


<div id="footer">
<div id="footer-text">
Page generated 2015-02-08 22:06:31 CDT, by <a href="http://jemdoc.jaboc.net/">jemdoc</a>.
</div>
</div>
</td>
</tr>
</table>
<!-- Start of StatCounter Code for Default Guide -->
<script type="text/javascript">
var sc_project=10281527; 
var sc_invisible=1; 
var sc_security="a2695fea"; 
var scJsHost = (("https:" == document.location.protocol) ?
"https://secure." : "http://www.");
document.write("<sc"+"ript type='text/javascript' src='" +
scJsHost+
"statcounter.com/counter/counter.js'></"+"script>");
</script>
<noscript><div class="statcounter"><a title="web analytics"
href="http://statcounter.com/" target="_blank"><img
class="statcounter"
src="http://c.statcounter.com/10281527/0/a2695fea/0/"
alt="web analytics"></a></div></noscript>
<!-- End of StatCounter Code for Default Guide -->
</body>
</html>
